# -*- coding: utf-8 -*-
"""Car_predecition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1exRvj0ngdSgQGgDrf8BMD8Xeb935Zds0

# **Car Price Prediction Model**
Developed for oasis Internship Task
This project builds a machine learning model to predict car prices based on various factors such as mileage,engine,fuel type. It includes:
 1.Data Ppreprocessing
2. Model Training & Hyperparameter tunning
3.Data Visualization & Model Evaluation

Author: Riya Pandey
"""

# Importing necessary libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
import sklearn.metrics as metrics
from sklearn.metrics import accuracy_score

'''Load and Explore the Dataset'''

# Load the dataset
file_path="/content/car data.csv"
df=pd.read_csv(file_path)

# Display the first few rows of dataset
display(df.head(5))

# Display column names,data types and missing/null values
display(df.info())

# Provide Summary Statsitics for numerical columns--helps understand data distribution
display(df.describe())

''' Data Preprocessing'''

# To handle missing/null values -- remove rows with null values
df=df.dropna()
df=pd.get_dummies(df,drop_first=True)

# Feature Scaling--normalize nonnumeric columns to numeric formate
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
numerical_cols=['Driven_kms']

df[numerical_cols]=scaler.fit_transform(df[numerical_cols])

''' Spltting The Data -- Dividing the dataset into features (X) and target variable (y)'''

X = df.drop(columns=['Selling_Price'])
y = df['Selling_Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

''' Train Machine Learning Models -- Linear regression and Random Forest'''

models={"Linear Regression": LinearRegression(),"Random Forest": RandomForestRegressor(n_estimators=100,random_state=42)} # Changed n_estimators from 00 to 100
for name,model in models.items():
  model.fit(X_train,y_train)
  y_pred=model.predict(X_test)

  print(name)
  print("MSE:",mean_squared_error(y_test,y_pred))
  print("MAE:",mean_absolute_error(y_test,y_pred))
  print("R2 score:",r2_score(y_test,y_pred))

''' Hyperparameter Tunning'''
# Using gridsearch to optimize the Random Forest

from sklearn.model_selection import GridSearchCV
param_grid = {'n_estimators': [50,100,200],'max_depth':[10,20,None]}
grid_search=GridSearchCV(RandomForestRegressor(),param_grid,cv=3,scoring='r2')
grid_search.fit(X_train,y_train)
best_model=grid_search.best_estimator_
y_pred=best_model.predict(X_test)
print("Best Parameters:",grid_search.best_params_)
print("R2 Score:",r2_score(y_test,y_pred))

''' Data Visualization'''

#Residual Plotting

from sklearn.metrics import mean_absolute_error
y_pred=best_model.predict(X_test)
residuals=y_test - y_pred
plt.figure(figsize=(10,6))
sns.histplot(residuals,kde=True,color="red")
plt.axvline(x=0,color="black",linestyle="dashed")
plt.title("Residual Distribution")
plt.xlabel("Residual (Actual - Predicted)")
plt.ylabel("Frequency")
plt.show()

#Actual vs Predicted prices

plt.figure(figsize=(10,6))
# Changed from plt.scatterplot to sns.scatterplot
sns.scatterplot(x=y_test,y=y_pred,color="blue",alpha=0.5)
plt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'r--',lw=2)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted Prices")
plt.show()

#Model Evaluation Metrices
r2=r2_score(y_test,y_pred)
mae=mean_absolute_error(y_test,y_pred)
mse=mean_squared_error(y_test,y_pred)
rmse=np.sqrt(mse)

print(f"R2 Score:{r2:.2f}")
print(f"Mean Absolute Error:{mae:.2f}")
print(f"Mean Squared Error:{mse:.2f}")
print(f"Root Mean Squared Error:{rmse:.2f}")

"""## **Final Insights**


# 1. We built a Car Price Prediction model using the Car Price Prediction dataset.

# 2. Two machine learning models were implemented:
 Linear Regression and Random Forest Regressor.

# 3. The dataset was preprocessed by selecting key attributes like Driven Kilometers (Mileage), Fuel Type, and Selling Price.

# 5. Random Forest Regressor outperformed Linear Regression with higher accuracy and lower error rates.

# 7. The Actual vs. Predicted Trials graph confirmed that the Random Forest model provides accurate price estimations.

# 8. Overall, the model effectively predicts car prices based on historical data and can be further optimized with additional features.


"""

import joblib
# Assign the model you want to save to the variable 'model'
model = best_model  # Changed 'rf' to 'model'

joblib.dump(model, "model.pkl")
print(" Model saved as model.pkl ")

pip freeze > requirements.txt